\begin{center}
    {\large {\bf ABSTRACT}}
\end{center}

Synthetic Aperture Radar (SAR) imagery, while highly informative in terms of structure and texture, suffers from a lack of visual interpretability due to its grayscale nature. This project presents a novel approach for colorizing SAR images by translating them into their optical equivalents using a Latent Diffusion Model (LDM). Leveraging the strengths of image-to-image translation with text guidance, the proposed method incorporates a multi-component architecture consisting of a Variational Autoencoder (VAE), a UNet-based denoising model, and CLIP for semantic conditioning. The dataset used comprises over 280,000 paired SAR and optical images from Sentinel-1 and Sentinel-2 missions, carefully curated with respect to satellite operational modes, polarization, seasonal variation, and temperature region classification. Extensive preprocessing and manual inspections ensured high-quality data preparation. The trained model effectively learns the mapping between SAR textures and plausible optical color representations, guided by auxiliary text inputs describing contextual attributes such as season and temperature zone. Results demonstrate the modelâ€™s ability to generate perceptually convincing and semantically consistent colorized outputs. This work opens new avenues for enhanced visual interpretation of SAR data, with potential applications in environmental monitoring, urban planning, and disaster assessment.

Complete Implementation and source code is available at Github: \url{https://github.com/ShambaC/SAR-Image-Colorisation}