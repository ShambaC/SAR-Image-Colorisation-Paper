There are several image colorization machine learning models available, e.g. - 
\begin{itemize}
    \item GAN
    \item CGAN
    \item DCGAN
    \item Pix2Pix GAN
    \item CycleGAN
    \item Latent Diffusion Model(LDM)
\end{itemize}

But to select the most appropriate image colorization model for our SAR image colorization, we need to look at the features, pros and cons of the available models and decide which model's features aligns with our goals the best.

\subsection{GAN}
GANs (Generative Adversarial Network) are a class of neural networks that autonomously learn patterns in the input data to generate new samples resembling the original dataset.
GAN's architecture consists of two neural networks:\par
    \textbf{\textit{Generator} -} Creates synthetic data from random noise to produce data so realistic that the discriminator cannot distinguish it from real data.\par
    \textbf{\textit{discriminator} -} Acts as a critic, evaluating whether the data it receives is real or fake.

\subsection{CGAN}
A Conditional Generative Adversarial Network (CGAN) is an advanced type of GAN (Generative Adversarial Network) where output generation is controlled using a condition.\par
For example, if you want to generate only Mercedes cars from a dataset of various cars, CGANs allow you to specify "Mercedes" as a condition.

\subsection{DCGAN}
DCGAN, ot Deep Convolutional Generative Adversarial Network uses a generator composed of a series of transpose convolution operations. These operations take in a random noise vector, z, and transform it by progressively increasing its spatial dimensions while decreasing its feature volume depth. The discriminator is basically a convolutional neural network which classifies an image as real or fake.

\subsection{Pix2Pix GAN}
It uses CGAN model and U-net architecture to perform image-to-image translations. But we can only provide images as input, no text can be provided as input prompt.

\subsection{CycleGAN}
In CycleGAN, we treat the problem as an image reconstruction problem. We first take an image input (x) and use the generator G to convert it into the transformed image. Then we reverse this process from transformed image to original image using a generator F. Then we calculate the mean squared error loss between real and reconstructed image.\par
The most important feature of this cycle GAN is that it can do this image translation on an unpaired image where there is no relation exists between the input image and output image.

\subsection{Latent Diffusion Model}
The key innovation of latent diffusion models is that they apply this diffusion process not to the raw pixel values of an image but instead to an encoded latent representation of the image.
How it Works:
\begin{itemize}
    \item \textbf{Encoding:} An input image is encoded by the autoencoder into a latent representation. 
    \item \textbf{Forward Diffusion:} The latent representation is progressively corrupted by adding noise. 
    \item \textbf{Reverse Diffusion:} The denoising U-Net iteratively removes the noise from the corrupted latent representation, guided by the conditioning input (e.g., text). 
    \item \textbf{Decoding:} The denoised latent representation is decoded by the autoencoder to produce the final generated image.
\end{itemize}

\section{Why latent diffusion model?} \label{sec:WhyLDM}

\textbf{Our Objective :} To train the model with paired images(black-and-white images and their corresponding colored images) and a text prompt, so that we can provide the model any new black-and-white image and get the corresponding colorized image.
\par A comparison chart of the previously discussed image colorization models is given below -

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Prompt Input} & \textbf{Image Quality} & \textbf{Required Paired Data} & \textbf{Stability} & \textbf{Control} \\
\hline
GAN & No & High & No & No & No \\
CGAN & Yes & High & Yes & No & Yes \\
DCGAN & No & Moderate & No & No & No \\
Pix2Pix GAN & Yes (Image) & Very High & Yes & Yes & Yes \\
CycleGAN & No & Moderate & No & Yes & No \\
LDM & Yes (Text/Image) & Very High & Yes & Yes & Yes \\
\hline
\end{tabular}
\caption{Comparison of deep generative models for SAR image colorization}
\label{tab:model_comparison}
\end{table}

As shown in Table~\ref{tab:model_comparison}, LDM offers the highest control, stability and image quality. It also supports text/image prompt input and training using paired data - which clearly satisfies our objective for SAR image colorization.\par
Also, by operating in this compressed latent space instead of directly on pixels, latent diffusion model offers several key advantages:
\begin{itemize}
    \item \textbf{Computationally efficient:} The smaller latent space representation makes executing the diffusion process much faster. This allows the models to be trained on consumer GPUs rather than requiring hundreds of GPUs like pixel-based diffusion models.
    \item \textbf{High resolution:} The decoder upsamples the modified latent code into a high-resolution output image, enabling manipulation of images at resolutions not possible with raw pixels.
    \item \textbf{Flexible conditioning:} Text, images, segmentation maps and other inputs can be encoded into the latent space and used to condition the model to generate outputs with desired characteristics.
    \item \textbf{Detail preservation:} The encoder-decoder structure allows for manipulating images while still preserving intricate details from the original inputs.
\end{itemize}

Hence, Latent Diffusion Model is the most suitable and appropriate choice for our project. 

\section{Components of LDM}

3 parts

\subsection{VAE}

Variational Auto Encoder

\subsection{CLIP}

\subsubsection{What is CLIP?}
Contrastive Languageâ€“Image Pretraining (CLIP) is a neural network model introduced by OpenAI that learns to connect images and text by training on a massive dataset of 400 million (image, text) pairs collected from the internet \cite{radford2021learningtransferablevisualmodels}. Unlike traditional supervised models that rely on task-specific labeled data, CLIP is trained with natural language supervision using a contrastive learning objective. Its dual-encoder structure includes both an image encoder and a text encoder, enabling CLIP to perform zero-shot transfer across a variety of vision tasks.

\subsubsection{Architecture of the Text Encoder}
The CLIP text encoder is a transformer-based architecture similar in design to GPT-2 \cite{radford2021learningtransferablevisualmodels}. It operates on a byte pair encoded (BPE) tokenized input with positional embeddings and uses masked self-attention. The encoded text is projected into a joint embedding space shared with image embeddings. Specifically, the final layer's output corresponding to the \texttt{[EOS]} token is extracted and linearly projected for contrastive learning alignment.

\subsubsection{Working Mechanism}
During training, CLIP is optimized to distinguish correct (image, text) pairs from incorrect ones using a symmetric InfoNCE loss. Given a batch of $N$ (image, text) pairs, the model learns embeddings such that the cosine similarity between matched pairs is maximized, and mismatches are minimized. This is done for both directions: image-to-text and text-to-image.

\subsubsection{Features and Capabilities}
\begin{itemize}
    \item \textbf{Zero-shot Learning}: CLIP performs tasks without any additional fine-tuning, simply by using textual prompts.
    \item \textbf{Flexible Prompting}: Users can guide CLIP with different natural language phrases, allowing contextual control over predictions.
    \item \textbf{Text Encoder for NLP}: Although trained for vision-language tasks, the CLIP text encoder can be repurposed for phrase understanding and even outperforms traditional language models like BERT in certain tasks \cite{yan2022clipunderstandstextprompting}.
    \item \textbf{Open-vocabulary Classification}: Since labels are given as text, CLIP can recognize classes it has never seen during training.
\end{itemize}

\subsubsection{Comparison with Other Models}
Unlike BERT, which uses masked language modeling, or GPT, which uses autoregressive prediction, CLIP uses a contrastive loss across modalities. This enables alignment between vision and language but limits the encoder's performance on typical NLP tasks unless prompted appropriately. Nonetheless, when evaluated on phrase understanding and entity tasks, CLIP often outperforms BERT and other specialized models \cite{yan2022clipunderstandstextprompting}.

\subsubsection{Why Use CLIP?}
CLIP excels in:
\begin{itemize}
    \item \textbf{Cross-modal understanding}, bridging visual and textual inputs.
    \item \textbf{Zero-shot generalization}, enabling use without task-specific data.
    \item \textbf{Scalability and flexibility}, through prompt engineering and contrastive training.
    \item \textbf{Compact design}, with a 38M parameter text encoder that performs competitively against much larger models \cite{yan2022clipunderstandstextprompting}.
\end{itemize}

In SAR image colorization, where we take black-and-white satellite images as input paired with a label (text prompt) to generate the corresponding colorized image, the ability to robustly understand and align textual descriptions with visual content is crucial. The CLIP text encoder provides an effective solution by mapping input prompts into a shared embedding space with images, enabling models to accurately associate textual labels with visual features. This makes it an excellent choice for text-to-image generation and conditioning tasks, particularly in remote sensing domains where annotation is limited and interpretability is key. With its scalable training strategy, strong generalization, and lightweight architecture, CLIP offers a compelling foundation for multimodal SAR-based applications.



\subsection{UNET}

UNET

\section{Training}

Model training

\section{Inference}

Inferencing results