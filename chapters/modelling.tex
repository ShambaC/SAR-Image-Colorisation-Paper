There are several image colorization machine learning models available, e.g. - 
\begin{itemize}
    \item GAN
    \item CGAN
    \item DCGAN
    \item Pix2Pix GAN
    \item CycleGAN
    \item Latent Diffusion Model(LDM)
\end{itemize}

But to select the most appropriate image colorization model for our SAR image colorization, we need to look at the features, pros and cons of the available models and decide which model's features aligns with our goals the best.

\subsection{GAN}
GANs (Generative Adversarial Network) are a class of neural networks that autonomously learn patterns in the input data to generate new samples resembling the original dataset.
GAN's architecture consists of two neural networks:\par
    \textbf{\textit{Generator} -} Creates synthetic data from random noise to produce data so realistic that the discriminator cannot distinguish it from real data.\par
    \textbf{\textit{discriminator} -} Acts as a critic, evaluating whether the data it receives is real or fake.

\subsection{CGAN}
A Conditional Generative Adversarial Network (CGAN) is an advanced type of GAN (Generative Adversarial Network) where output generation is controlled using a condition.\par
For example, if you want to generate only Mercedes cars from a dataset of various cars, CGANs allow you to specify "Mercedes" as a condition.

\subsection{DCGAN}
DCGAN, ot Deep Convolutional Generative Adversarial Network uses a generator composed of a series of transpose convolution operations. These operations take in a random noise vector, z, and transform it by progressively increasing its spatial dimensions while decreasing its feature volume depth. The discriminator is basically a convolutional neural network which classifies an image as real or fake.

\subsection{Pix2Pix GAN}
It uses CGAN model and U-net architecture to perform image-to-image translations. But we can only provide images as input, no text can be provided as input prompt.

\subsection{CycleGAN}
In CycleGAN, we treat the problem as an image reconstruction problem. We first take an image input (x) and use the generator G to convert it into the transformed image. Then we reverse this process from transformed image to original image using a generator F. Then we calculate the mean squared error loss between real and reconstructed image.\par
The most important feature of this cycle GAN is that it can do this image translation on an unpaired image where there is no relation exists between the input image and output image.

\subsection{Latent Diffusion Model}
The key innovation of latent diffusion models is that they apply this diffusion process not to the raw pixel values of an image but instead to an encoded latent representation of the image.
How it Works:
\begin{itemize}
    \item \textbf{Encoding:} An input image is encoded by the autoencoder into a latent representation. 
    \item \textbf{Forward Diffusion:} The latent representation is progressively corrupted by adding noise. 
    \item \textbf{Reverse Diffusion:} The denoising U-Net iteratively removes the noise from the corrupted latent representation, guided by the conditioning input (e.g., text). 
    \item \textbf{Decoding:} The denoised latent representation is decoded by the autoencoder to produce the final generated image.
\end{itemize}

\section{Why latent diffusion model?}

\textbf{Our Objective :} To train the model with paired images(black-and-white images and their corresponding colored images) and a text prompt, so that we can provide the model any new black-and-white image and get the corresponding colorized image.
\par A comparison chart of the previously discussed image colorization models is given below -

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Prompt Input} & \textbf{Image Quality} & \textbf{Required Paired Data} & \textbf{Stability} & \textbf{Control} \\
\hline
GAN & No & High & No & No & No \\
CGAN & Yes & High & Yes & No & Yes \\
DCGAN & No & Moderate & No & No & No \\
Pix2Pix GAN & Yes (Image) & Very High & Yes & Yes & Yes \\
CycleGAN & No & Moderate & No & Yes & No \\
LDM & Yes (Text/Image) & Very High & Yes & Yes & Yes \\
\hline
\end{tabular}
\caption{Comparison of deep generative models for SAR image colorization}
\label{tab:model_comparison}
\end{table}

As shown in Table~\ref{tab:model_comparison}, LDM offers the highest control, stability and image quality. It also supports text/image prompt input and training using paired data - which clearly satisfies our objective for SAR image colorization.\par
Also, by operating in this compressed latent space instead of directly on pixels, latent diffusion model offers several key advantages:
\begin{itemize}
    \item \textbf{Computationally efficient:} The smaller latent space representation makes executing the diffusion process much faster. This allows the models to be trained on consumer GPUs rather than requiring hundreds of GPUs like pixel-based diffusion models.
    \item \textbf{High resolution:} The decoder upsamples the modified latent code into a high-resolution output image, enabling manipulation of images at resolutions not possible with raw pixels.
    \item \textbf{Flexible conditioning:} Text, images, segmentation maps and other inputs can be encoded into the latent space and used to condition the model to generate outputs with desired characteristics.
    \item \textbf{Detail preservation:} The encoder-decoder structure allows for manipulating images while still preserving intricate details from the original inputs.
\end{itemize}

Hence, Latent Diffusion Model is the most suitable and appropriate choice for our project. 

\section{Components of LDM}

3 parts

\subsection{VAE}

Variational Auto Encoder

\subsection{CLIP}

Contrastive Languageâ€“Image Pre-training

\subsection{UNET}

UNET

\section{Training}

Model training

\section{Inference}

Inferencing results